{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V0_IBpbi8UCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054a89ee-3c09-498a-d498-b2b8f1d40e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sApV9Y658wh8"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/drive/MyDrive/archive.zip\" -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder for Image Enhancement: Training and Visualization\n",
        "\n",
        "In this project, we develop a Variational Autoencoder (VAE) model using PyTorch to enhance underwater images. The model is designed to denoise images by applying a forward diffusion process, followed by a reverse process using a U-Net architecture. This write-up outlines the key components of the implementation, including data preparation, model architecture, training, and visualization of results.\n",
        "\n",
        "## 1. Data Preparation\n",
        "\n",
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "ssCFBEMAyqTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L7oxeGSOCdxg"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Training Data\n",
        "\n",
        "The training data consists of raw images located in the specified directory. We utilize the `ImageDataGenerator` class from TensorFlow to load and preprocess the images, including normalization to the range [0, 1]. The images are resized to 224x224 pixels for uniformity."
      ],
      "metadata": {
        "id": "vRM8NPSHzS63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O3yn-LcUCNeA"
      },
      "outputs": [],
      "source": [
        "# Set paths for training and reference directories\n",
        "train_dir = '/content/Train'  # Update this to your train directory path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTp3_cDLCS0a",
        "outputId": "bbc7d18a-6e45-4ef8-8363-9809f48c4152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1400 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Initialize ImageDataGenerators\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "# Load images from train directory (for raw images)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='input',\n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQGeANfxCVNC",
        "outputId": "95d74a70-6cda-4314-f21f-debc6d1fc587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1400, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "# Initialize empty lists to hold image data\n",
        "X_train = []\n",
        "\n",
        "# Get number of samples\n",
        "num_train_samples = train_generator.n\n",
        "\n",
        "\n",
        "# Iterate through the train generator and store images in X_train\n",
        "for batch in train_generator:\n",
        "    X_train.append(batch[0])  # Append the first element which contains the image data\n",
        "    if len(X_train) * train_generator.batch_size >= num_train_samples:\n",
        "        break\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_train = np.concatenate(X_train, axis=0)\n",
        "\n",
        "\n",
        "# Normalize images\n",
        "X_train = (X_train - 127.5) / 127.5  # Scale to [-1, 1]\n",
        "\n",
        "\n",
        "# Output shapes\n",
        "print(f\"X_train shape: {X_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1khDiCOgtTm",
        "outputId": "f5e83186-2e49-494e-df17-8d2e8e513f8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.7 torchmetrics-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Defining the Dataset Class\n",
        "\n",
        "We define a custom dataset class, `UnderwaterDataset`, that inherits from PyTorch’s `Dataset` class. This class enables us to handle our image data efficiently and provides methods for accessing the dataset.\n"
      ],
      "metadata": {
        "id": "e2CqpYXgz6Qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R6TZsDz_CgvT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "\n",
        "\n",
        "# Define a custom dataset for underwater images\n",
        "class UnderwaterDataset(Dataset):\n",
        "    def __init__(self, X_train):\n",
        "        self.data = X_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Transform image to match PyTorch's format (C, H, W)\n",
        "        image = self.data[idx].transpose((2, 0, 1))  # From (H, W, C) to (C, H, W)\n",
        "        image = torch.tensor(image, dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
        "        return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Forward Diffusion Process\n",
        "\n",
        "We implement the forward diffusion process, which adds noise to the images to simulate the effect of corruption. This step is crucial for training our model to learn how to denoise images effectively."
      ],
      "metadata": {
        "id": "Z7Je9rENz_13"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sLRQP3_zC3AC"
      },
      "outputs": [],
      "source": [
        "#Modify the forward diffusion process for underwater images\n",
        "def forward_diffusion(x0, noise, t, T):\n",
        "    \"\"\"\n",
        "    Applies the forward diffusion process.\n",
        "\n",
        "    Args:\n",
        "    - x0: Original image tensor (batch_size, channels, height, width)\n",
        "    - noise: Gaussian noise tensor with the same shape as x0\n",
        "    - t: Current time step in the diffusion process\n",
        "    - T: Total number of time steps\n",
        "\n",
        "    Returns:\n",
        "    - xt: Noised image tensor at time step t\n",
        "    \"\"\"\n",
        "    alpha = 1 - (t / T)  # Simple linear schedule\n",
        "    return alpha * x0 + (1 - alpha) * noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. U-Net Architecture for Denoising\n",
        "\n",
        "A simple U-Net-like model is defined to reconstruct the original images from the noisy versions. The U-Net architecture consists of an encoder for downsampling and a decoder for upsampling."
      ],
      "metadata": {
        "id": "EYWX_q7O0DaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sEdZBc6rC7_y"
      },
      "outputs": [],
      "source": [
        "# Simple U-Net-like model for reverse process (denoising)\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder (downsampling)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder (upsampling)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()  # Output in range [0, 1] for image generation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Configuration\n",
        "\n",
        "The training hyperparameters, including the number of time steps (T), batch size, learning rate, and number of epochs, are defined. The model is moved to the appropriate device (GPU or CPU)"
      ],
      "metadata": {
        "id": "qifMA-xa0HuS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cpr1HZa0DGPD"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "T = 1000  # Number of time steps\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "epochs = 10\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load your underwater dataset (assuming X_train is provided with shape [1400, 224, 224, 3])\n",
        "# X_train = np.load(\"X_train.npy\")  # Uncomment if loading from file\n",
        "dataset = UnderwaterDataset(X_train=X_train)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, optimizer, loss\n",
        "model = SimpleUNet(in_channels=3, out_channels=3)  # RGB images have 3 channels\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S6tV5WqgERFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0101da-c3d8-4108-f9b4-0b3962cbd61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
            "  _future_warning(\n"
          ]
        }
      ],
      "source": [
        "# Initialize SSIM metric and move it to the same device as the model\n",
        "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)  # Set data_range to 1 since images are normalized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Loop\n",
        "\n",
        "The training loop iterates through the dataset for the specified number of epochs. In each iteration, noise is added to the images using the forward diffusion process. The model attempts to reconstruct the original images from the noisy input. The Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM) are calculated to evaluate performance.\n"
      ],
      "metadata": {
        "id": "EpQu2SSA0Ke3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GdGoJoWDKFG",
        "outputId": "a17f3e99-cb56-45d8-81c7-47c42e246e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 -> MSE: 0.0323, PSNR: 31.0294, SSIM: 0.3233\n",
            "Epoch 2/10 -> MSE: 0.0001, PSNR: 43.0015, SSIM: 0.6600\n",
            "Epoch 3/10 -> MSE: 0.0001, PSNR: 45.0177, SSIM: 0.7671\n",
            "Epoch 4/10 -> MSE: 0.0000, PSNR: 45.7023, SSIM: 0.8041\n",
            "Epoch 5/10 -> MSE: 0.0000, PSNR: 46.3924, SSIM: 0.8308\n",
            "Epoch 6/10 -> MSE: 0.0000, PSNR: 46.7783, SSIM: 0.8419\n",
            "Epoch 7/10 -> MSE: 0.0000, PSNR: 47.0420, SSIM: 0.8486\n",
            "Epoch 8/10 -> MSE: 0.0000, PSNR: 47.2165, SSIM: 0.8524\n",
            "Epoch 9/10 -> MSE: 0.0000, PSNR: 47.4216, SSIM: 0.8563\n",
            "Epoch 10/10 -> MSE: 0.0000, PSNR: 47.4844, SSIM: 0.8577\n",
            "\n",
            "Training Complete! Final Scores:\n",
            "Average MSE over all epochs: 0.0033\n",
            "Average PSNR over all epochs: 44.7086\n",
            "Average SSIM over all epochs: 0.7642\n"
          ]
        }
      ],
      "source": [
        "# Variables to track losses\n",
        "mse_losses = []\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_mse = 0\n",
        "    epoch_psnr = 0\n",
        "    epoch_ssim = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        data = data.to(device)  # Move input data to the same device\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Apply forward diffusion (add noise)\n",
        "        noise = torch.randn_like(data)\n",
        "        t = torch.randint(0, T, (data.shape[0],)).view(-1, 1, 1, 1).to(device)  # Random time steps, reshaped to broadcast\n",
        "        xt = forward_diffusion(data, noise, t, T)\n",
        "\n",
        "        # Denoising step: predict original image from noisy image\n",
        "        reconstructed = model(xt)\n",
        "        loss = criterion(reconstructed, data)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate losses and metrics\n",
        "        mse_loss = loss.item()\n",
        "        epoch_mse += mse_loss\n",
        "\n",
        "        # PSNR calculation using original PyTorch tensors\n",
        "        batch_psnr = 0\n",
        "        for i in range(data.size(0)):  # Calculate PSNR per image in the batch\n",
        "            orig = data[i].cpu().detach().numpy().transpose(1, 2, 0)\n",
        "            recon = reconstructed[i].cpu().detach().numpy().transpose(1, 2, 0)\n",
        "            batch_psnr += compare_psnr(orig, recon, data_range=1.0)\n",
        "        epoch_psnr += batch_psnr / data.size(0)\n",
        "\n",
        "        # SSIM using the PyTorch metric (batch-wise)\n",
        "        epoch_ssim += ssim_metric(reconstructed, data).item()\n",
        "\n",
        "    # Average the losses and metrics over the batches\n",
        "    epoch_mse /= len(train_loader)\n",
        "    epoch_psnr /= len(train_loader)\n",
        "    epoch_ssim /= len(train_loader)\n",
        "\n",
        "    mse_losses.append(epoch_mse)\n",
        "    psnr_scores.append(epoch_psnr)\n",
        "    ssim_scores.append(epoch_ssim)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs} -> MSE: {epoch_mse:.4f}, PSNR: {epoch_psnr:.4f}, SSIM: {epoch_ssim:.4f}')\n",
        "\n",
        "# Final Summary of Losses\n",
        "print(\"\\nTraining Complete! Final Scores:\")\n",
        "print(f\"Average MSE over all epochs: {np.mean(mse_losses):.4f}\")\n",
        "print(f\"Average PSNR over all epochs: {np.mean(psnr_scores):.4f}\")\n",
        "print(f\"Average SSIM over all epochs: {np.mean(ssim_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Visualization of Generated Images\n",
        "\n",
        "To evaluate the visual output of the trained model, we generate five images from random noise and visualize them. This step helps to understand the quality of the denoised images produced by the model."
      ],
      "metadata": {
        "id": "IlxDY30K0M_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qvL_TVh4DN9i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "53b31f2c-e7c2-4486-beb0-9776ff0dd4d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# (Previous code for training the model here...)\n",
        "\n",
        "# After training, visualize the first five generated images\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    noise = torch.randn(5, 3, 224, 224).to(device)  # Generate random noise for 5 images\n",
        "    generated_images = model(noise)  # Denoise using the trained model\n",
        "\n",
        "# Move generated images to CPU and convert to numpy for visualization\n",
        "generated_images = generated_images.cpu().numpy()\n",
        "\n",
        "# Denormalize the images to the range [0, 255] for visualization\n",
        "generated_images = np.clip(generated_images * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Visualize the first five images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))  # Create a row of 5 plots\n",
        "for i, ax in enumerate(axes):\n",
        "    # Each image is (C, H, W) - transpose to (H, W, C) for visualization\n",
        "    image = generated_images[i].transpose(1, 2, 0)\n",
        "    ax.imshow(image)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Generated Image {i+1}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Model Performance Evaluation\n",
        "\n",
        "After completing the training of our Variational Autoencoder (VAE) model designed for enhancing underwater images, we can evaluate its performance using several key metrics: Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). These metrics provide a comprehensive understanding of the model's effectiveness in reconstructing the original images from noisy inputs.\n",
        "\n",
        "## 1. Training Results Summary\n",
        "\n",
        "The training process concluded with the following average scores over all epochs:\n",
        "\n",
        "- **Average MSE**: 0.0033\n",
        "- **Average PSNR**: 44.7086\n",
        "- **Average SSIM**: 0.7642\n",
        "\n",
        "### 1.1 Mean Squared Error (MSE)\n",
        "\n",
        "The **Mean Squared Error** (MSE) is a critical metric that quantifies the average squared difference between the predicted images and the original images. A lower MSE indicates that the model's predictions are closer to the true values.\n",
        "\n",
        "- **Final Average MSE**: **0.0033**\n",
        "  - This value suggests that, on average, the differences between the reconstructed images and the original images are minimal. The low MSE reflects the model’s ability to effectively denoise and reconstruct the images, retaining most of the original details.\n",
        "\n",
        "### 1.2 Peak Signal-to-Noise Ratio (PSNR)\n",
        "\n",
        "The **Peak Signal-to-Noise Ratio** (PSNR) is a logarithmic measure used to evaluate the quality of reconstructed images compared to the original images. It is calculated based on the ratio between the maximum possible power of a signal (the image) and the power of corrupting noise. Higher PSNR values generally indicate better quality reconstructions.\n",
        "\n",
        "- **Final Average PSNR**: **44.7086 dB**\n",
        "  - A PSNR value above 40 dB is typically considered indicative of excellent quality. In our case, an average PSNR of 44.7086 dB suggests that the reconstructed images maintain a high level of fidelity, with very little perceptible difference from the original images. This high PSNR reinforces the model's effectiveness in denoising tasks.\n",
        "\n",
        "### 1.3 Structural Similarity Index Measure (SSIM)\n",
        "\n",
        "The **Structural Similarity Index Measure** (SSIM) is an important metric for assessing the visual quality of images. Unlike MSE and PSNR, SSIM takes into account changes in structural information, luminance, and contrast. The values of SSIM range from -1 to 1, with 1 indicating perfect structural similarity.\n",
        "\n",
        "- **Final Average SSIM**: **0.7642**\n",
        "  - An SSIM value of 0.7642 indicates a reasonably high degree of structural similarity between the reconstructed and original images. While not perfect, this score suggests that the model successfully preserves important structural features of the images, resulting in visually coherent outputs.\n",
        "\n",
        "## 2. Conclusion\n",
        "\n",
        "The training results of our diffusion model demonstrate its capability to effectively denoise underwater images. The low average MSE, high PSNR, and reasonably strong SSIM collectively illustrate the model's performance, indicating that it not only minimizes noise but also retains crucial structural details in the reconstructed images. These outcomes confirm that the diffusion model is a robust solution for image enhancement in underwater environments, setting a solid foundation for potential real-world applications."
      ],
      "metadata": {
        "id": "Gt-IPsnN0ip_"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}