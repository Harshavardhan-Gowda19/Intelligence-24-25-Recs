{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfaaf38-621f-4392-b047-03424ef2188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for the project\n",
    "import os\n",
    "\n",
    "base_path = \"HogwartsQ&A\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'data'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'api'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'web'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_path, 'static'), exist_ok=True)\n",
    "\n",
    "print(f\"Project folder structure created at: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407c5dd1-81f8-440c-baae-b0fc2ac8bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the book from a URL (if accessible)\n",
    "import requests\n",
    "\n",
    "url = 'https://ia902903.us.archive.org/12/items/FantasyFictionebookcollection/Harry%20Potter/3%20-%20Harry%20Potter%20and%20the%20Prisoner%20of%20Azkaban.pdf'\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(base_path, 'data', 'Harry_Potter_and_Prisoner_of_Azkaban.pdf'), 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d97d3a-23ca-40fa-b9a0-ba10089d7290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pdfminer.six) (42.0.5)\n",
      "Collecting transformers<5.0.0,>=4.38.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.19.3 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi)\n",
      "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi) (2.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.14.6)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from starlette<0.39.0,>=0.37.2->fastapi) (4.2.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.10.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.38.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.38.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0-cp312-cp312-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.8/381.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uvicorn, safetensors, faiss-cpu, torch, starlette, huggingface-hub, tokenizers, pdfminer.six, fastapi, transformers, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.9.0 fastapi-0.115.0 huggingface-hub-0.25.2 pdfminer.six-20240706 safetensors-0.4.5 sentence-transformers-3.1.1 starlette-0.38.6 tokenizers-0.20.0 torch-2.4.1 transformers-4.45.2 uvicorn-0.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six sentence-transformers faiss-cpu fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20882016-4b9a-4312-8b5d-aa8ffc78de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF parsed and saved as text successfully!\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Define file paths\n",
    "pdf_path = f\"{base_path}/data/Harry_Potter_and_Prisoner_of_Azkaban.pdf\"\n",
    "text_output_path = f\"{base_path}/data/Harry_Potter_Parsed.txt\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "# Save the parsed text to a file\n",
    "with open(text_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"PDF parsed and saved as text successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c6020f-7022-47f8-a019-1260f4eddb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Harry Potter and the Prisoner of Azkaban by J. K. Rowling Illustrations By Mary Grandpré Arthur A. Levine Books An Imprint of Scholastic Press. To Jill Prewett and Aine Kiely, the GodMothers of Swing ...\n",
      "\n",
      "Chunk 2: Rowling, J. K. Harry Potter and the Prisoner of Azkaban / by J. K. Rowling. p. cm. Sequel to: Harry Potter and the Chamber of Secrets Summary: During his third year at Hogwarts School for Witchcraft a...\n",
      "\n",
      "Chunk 3: midnight, and he was lying on his stomach in bed, the blankets drawn right over his head like a tent, a ﬂashlight in one hand and a large leather-bound book (A History of Magic by Bathilda Bagshot) pr...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, chunk_size=150):\n",
    "    \"\"\"Splits the text into smaller chunks based on word count.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Read the parsed text file\n",
    "with open(text_output_path, 'r', encoding='utf-8') as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "# Split the text into chunks of 150 words each\n",
    "text_chunks = split_text_into_chunks(book_text)\n",
    "\n",
    "# Show a few sample chunks\n",
    "for i, chunk in enumerate(text_chunks[:3]):\n",
    "    print(f\"Chunk {i+1}: {chunk[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e4a804d-fa1a-4775-a15e-1d18cb1ec309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e4e629-a869-45da-a737-b6a421bd92e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc09d062841491b8b03124343925f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14844af1208471faab85b9ebfcccd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6986b26e632d453b928617d9342ace8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb071baa2b4f179b26e8047209d4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca92a1bbf87742a1a741f36ce65e8577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811a4dbe9cf1444aa2eb9213c158e0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba10011c423045ebb8ec897f624a9c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a9cd9abbc4470bb6902892d5e45a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2267f27056934be98c1ad1ff747c91c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405e05cf03494e728d9747cf2e955b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae20070bbd645b2a731f273d92ae480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and saved embeddings for 733 text chunks.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = model.encode(text_chunks)\n",
    "\n",
    "# Save the embeddings and chunks for future use\n",
    "with open(f\"{base_path}/models/book_embeddings.pkl\", 'wb') as f:\n",
    "    pickle.dump((text_chunks, embeddings), f)\n",
    "\n",
    "print(f\"Generated and saved embeddings for {len(embeddings)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "965fccd6-9684-4cfe-a1ed-276fff95e659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: /Users/harshavardhangowda/HogwartsQ&A/models/book_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '/Users/harshavardhangowda/HogwartsQ&A/models/book_embeddings.pkl'\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File found at: {file_path}\")\n",
    "else:\n",
    "    print(\"File not found. Please check the path or regenerate the embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5f2e615-2de0-40d6-9946-90642ed50ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 733 text chunks in FAISS.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_np = np.array(embeddings, dtype='float32')\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])  # L2 distance measure\n",
    "index.add(embeddings_np)  # Add the embeddings to the index\n",
    "\n",
    "print(f\"Indexed {len(embeddings)} text chunks in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dc7f0b4-4681-40aa-a2d5-3e665670e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: began to spread like a spider’s web from the point that George’s wand had touched. They joined each other, they crisscrossed, they fanned into every corner of the parchment; then words began to blosso | Score: 0.8207749128341675\n",
      "Text: up, Remus,” snarled Black, who was still watching Scabbers with a horrible sort of hunger on his face. “I’m getting there, Sirius, I’m getting there … well, highly exciting possibilities were open to  | Score: 1.021514654159546\n",
      "Text: said Lupin shortly. He glanced around the empty entrance hall and lowered his voice. “I happen to know that this map was conﬁscated by Mr. Filch many years ago. Yes, I know it’s a map,” he said as Har | Score: 1.0358514785766602\n",
      "Text: right … that’s why we called him Prongs.” Lupin threw his last few books into his case, closed the desk drawers, and turned to look at Harry. “Here — I brought this from the Shrieking Shack last night | Score: 1.0827686786651611\n",
      "Text: the heading of the map. “We owe them so much.” “Noble men, working tirelessly to help a new generation of law-breakers,” said Fred solemnly. “Right,” said George briskly. “Don’t forget to wipe it afte | Score: 1.1999804973602295\n"
     ]
    }
   ],
   "source": [
    "def search_chunks(query, index, model, chunks, top_n=5):\n",
    "    \"\"\"Searches for the most relevant chunks for a given query.\"\"\"\n",
    "    query_embedding = model.encode([query])[0]  # Encode the query\n",
    "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), top_n)\n",
    "    \n",
    "    # Retrieve top N relevant chunks\n",
    "    results = [{\"text\": chunks[idx], \"score\": distances[0][i]} for i, idx in enumerate(indices[0])]\n",
    "    return results\n",
    "\n",
    "# Test the search with a sample query\n",
    "query = \"Who created the Marauder's Map?\"\n",
    "results = search_chunks(query, index, model, text_chunks)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"Text: {result['text'][:200]} | Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc95bec-f1a1-4edd-abdb-b76e11af5ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What is the significance of the Marauder's Map?\", 'answer': 'began to spread like a spider’s web from the point that George’s wand had touched. They joined each other, they crisscrossed, they fanned into every corner of the parchment; then words began to blossom across the top, great, curly green words, that proclaimed: Messrs. Moony, Wormtail, Padfoot, and Prongs Purveyors of Aids to Magical Mischief-Makers are proud to present THE MARAUDER’S MAP in the top It was a map showing every detail of the Hogwarts castle and grounds. But the truly remarkable thing were the tiny ink dots moving around it, each labeled with a name in minuscule writing. Astounded, Harry bent over it. A labeled left corner showed that Professor dot Dumbledore was pacing his study; the caretaker’s cat, Mrs. Norris, was prowling the second ﬂoor; and Peeves the Poltergeist was currently bouncing around the trophy room. And as Harry’s eyes traveled up and down the familiar corridors, he'}\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Simulates the API response by using the search function.\"\"\"\n",
    "    results = search_chunks(question, index, model, text_chunks)\n",
    "    return {\"question\": question, \"answer\": results[0]['text'] if results else \"No relevant answer found.\"}\n",
    "\n",
    "# Test with a question\n",
    "response = ask_question(\"What is the significance of the Marauder's Map?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b956e3ec-b2a4-440e-b9e5-ccf2f2ea6030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: began to spread like a spider’s web from the point that George’s wand had touched. They joined each other, they crisscrossed, they fanned into every corner of the parchment; then words began to blossom across the top, great, curly green words, that proclaimed: Messrs. Moony, Wormtail, Padfoot, and Prongs Purveyors of Aids to Magical Mischief-Makers are proud to present THE MARAUDER’S MAP in the top It was a map showing every detail of the Hogwarts castle and grounds. But the truly remarkable thing were the tiny ink dots moving around it, each labeled with a name in minuscule writing. Astounded, Harry bent over it. A labeled left corner showed that Professor dot Dumbledore was pacing his study; the caretaker’s cat, Mrs. Norris, was prowling the second ﬂoor; and Peeves the Poltergeist was currently bouncing around the trophy room. And as Harry’s eyes traveled up and down the familiar corridors, he\n",
      "\n",
      "Additional Information:\n",
      "Chunk 2: up, Remus,” snarled Black, who was still watching Scabbers with a horrible sort of hunger on his face. “I’m getting there, Sirius, I’m getting there … well, highly exciting possibilities were open to ... | Score: 1.021514654159546\n",
      "Chunk 3: said Lupin shortly. He glanced around the empty entrance hall and lowered his voice. “I happen to know that this map was conﬁscated by Mr. Filch many years ago. Yes, I know it’s a map,” he said as Har... | Score: 1.0358514785766602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def query_pipeline(query, model, index, chunks, top_n=3):\n",
    "    \"\"\"Handles the end-to-end query pipeline for retrieving the most relevant chunks.\"\"\"\n",
    "    # Step 1: Convert the query to an embedding\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Step 2: Perform similarity search using FAISS index\n",
    "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), top_n)\n",
    "\n",
    "    # Step 3: Retrieve top N relevant chunks\n",
    "    top_chunks = [{\"text\": chunks[idx], \"score\": distances[0][i]} for i, idx in enumerate(indices[0])]\n",
    "    \n",
    "    # Step 4: Format the final response using the most relevant chunk\n",
    "    formatted_response = f\"Answer: {top_chunks[0]['text']}\\n\\nAdditional Information:\\n\"\n",
    "    for i, chunk in enumerate(top_chunks[1:], start=2):\n",
    "        formatted_response += f\"Chunk {i}: {chunk['text'][:200]}... | Score: {chunk['score']}\\n\"\n",
    "    \n",
    "    return formatted_response\n",
    "\n",
    "# Test the query pipeline with a sample question\n",
    "sample_question = \"Who created the Marauder's Map?\"\n",
    "response = query_pipeline(sample_question, model, index, text_chunks)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
